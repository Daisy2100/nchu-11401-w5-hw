{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8uGVFd2du6-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_var(x):\n",
        "  if torch.cuda.is_available():\n",
        "    x=x.cuda()\n",
        "  return Variable(x)"
      ],
      "metadata": {
        "id": "kOnoH9P866wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "jtGxi8oS66y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=0.5,\n",
        "                          std=0.5)])"
      ],
      "metadata": {
        "id": "sle-lwjO7unn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dsets.MNIST(root='./data',\n",
        "              train=True,\n",
        "              transform=transform,\n",
        "              download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data',\n",
        "              train=False,\n",
        "              transform=transform)"
      ],
      "metadata": {
        "id": "jJHoWWK06607"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as Data"
      ],
      "metadata": {
        "id": "ojfCImtw663K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100"
      ],
      "metadata": {
        "id": "dps-e9if665R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = Data.DataLoader(dataset=train_dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True)\n"
      ],
      "metadata": {
        "id": "Wq0XXePg67ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "dhI5761U67j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D = nn.Sequential(\n",
        "    nn.Linear(28*28, 256),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Linear(256, 256),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Linear(256, 1),\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "metadata": {
        "id": "z12i_F5X67ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = nn.Sequential(\n",
        "    nn.Linear(64, 256),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Linear(256, 256),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Linear(256, 28*28),\n",
        "    nn.Tanh()\n",
        ")"
      ],
      "metadata": {
        "id": "SJnA2zzI67oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  D.cuda()\n",
        "  G.cuda()\n"
      ],
      "metadata": {
        "id": "lwVHvydj67qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "d_opt = torch.optim.Adam(D.parameters(), lr=0.0003)\n",
        "g_opt = torch.optim.Adam(G.parameters(), lr=0.0003)"
      ],
      "metadata": {
        "id": "cGLDa1CV_P-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import save_image"
      ],
      "metadata": {
        "id": "0j89dIJ__QAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def denorm(x):\n",
        "  out = (x+1)/2\n",
        "  out.clamp_(0,1)\n",
        "  return out"
      ],
      "metadata": {
        "id": "bGZy-9N-U4Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(200):\n",
        "  for i, (images, _) in enumerate(data_loader):\n",
        "    batch_size = images.size(0)\n",
        "    images = to_var(images.view(batch_size, -1))\n",
        "\n",
        "    real_labels = to_var(torch.ones(batch_size, 1))\n",
        "    fake_labels = to_var(torch.zeros(batch_size, 1))\n",
        "\n",
        "    outputs = D(images)\n",
        "    d_loss_real = loss_fn(outputs, real_labels)\n",
        "    real_score = outputs\n",
        "\n",
        "    z = to_var(torch.randn(batch_size, 64))\n",
        "    fake_images = G(z)\n",
        "    outputs = D(fake_images)\n",
        "    d_loss_fake = loss_fn(outputs, fake_labels)\n",
        "    fake_score = outputs\n",
        "\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "    D.zero_grad()\n",
        "    d_loss.backward()\n",
        "    d_opt.step()\n",
        "\n",
        "    z = to_var(torch.randn(batch_size, 64))\n",
        "    fake_images = G(z)\n",
        "    outputs = D(fake_images)\n",
        "\n",
        "    g_loss = loss_fn(outputs, real_labels)\n",
        "    D.zero_grad()\n",
        "    G.zero_grad()\n",
        "    g_loss.backward()\n",
        "    g_opt.step()\n",
        "\n",
        "    if (i+30)%300 ==0 :\n",
        "      print(\"Epoch %d, batch %d, d_loss: %.4f, g_loss: %.4f,\"\n",
        "      \"D(x): %.2f, D(G(z)): %.2f\"\n",
        "      %(epoch, i+1, d_loss.data, g_loss.data,\n",
        "        real_score.data.mean(), fake_score.data.mean()))\n",
        "\n",
        "  if (epoch ==0):\n",
        "    images = images.view(batch_size, 1, 28, 28)\n",
        "    save_image(denorm(images), \"./data/real_images.png\")\n",
        "  fake_images = fake_images.view(fake_images.size(0), 1, 28, 28)\n",
        "  save_image(denorm(fake_images), \"./data/fake_images-%d.png\"%(epoch+1))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA0ra7AB_QCu",
        "outputId": "59139d88-5e4b-4de2-b6be-44817d3d788d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, batch 271, d_loss: 0.9523, g_loss: 3.3826,D(x): 0.71, D(G(z)): 0.30\n",
            "Epoch 0, batch 571, d_loss: 0.1914, g_loss: 3.3210,D(x): 0.97, D(G(z)): 0.15\n",
            "Epoch 1, batch 271, d_loss: 1.6304, g_loss: 3.2802,D(x): 0.85, D(G(z)): 0.49\n",
            "Epoch 1, batch 571, d_loss: 0.7206, g_loss: 6.0111,D(x): 0.77, D(G(z)): 0.11\n",
            "Epoch 2, batch 271, d_loss: 0.3023, g_loss: 4.5459,D(x): 0.88, D(G(z)): 0.09\n",
            "Epoch 2, batch 571, d_loss: 1.6474, g_loss: 1.0663,D(x): 0.56, D(G(z)): 0.50\n",
            "Epoch 3, batch 271, d_loss: 1.9637, g_loss: 0.6577,D(x): 0.46, D(G(z)): 0.55\n",
            "Epoch 3, batch 571, d_loss: 0.7762, g_loss: 1.6382,D(x): 0.74, D(G(z)): 0.28\n",
            "Epoch 4, batch 271, d_loss: 0.3329, g_loss: 3.4993,D(x): 0.87, D(G(z)): 0.13\n",
            "Epoch 4, batch 571, d_loss: 0.8928, g_loss: 2.8431,D(x): 0.77, D(G(z)): 0.27\n",
            "Epoch 5, batch 271, d_loss: 0.5828, g_loss: 2.8135,D(x): 0.84, D(G(z)): 0.23\n",
            "Epoch 5, batch 571, d_loss: 0.4490, g_loss: 2.1374,D(x): 0.87, D(G(z)): 0.22\n",
            "Epoch 6, batch 271, d_loss: 0.7843, g_loss: 2.5340,D(x): 0.77, D(G(z)): 0.31\n",
            "Epoch 6, batch 571, d_loss: 0.8740, g_loss: 1.7273,D(x): 0.74, D(G(z)): 0.35\n",
            "Epoch 7, batch 271, d_loss: 1.0673, g_loss: 1.9355,D(x): 0.63, D(G(z)): 0.28\n",
            "Epoch 7, batch 571, d_loss: 0.8372, g_loss: 2.1842,D(x): 0.76, D(G(z)): 0.32\n",
            "Epoch 8, batch 271, d_loss: 0.3323, g_loss: 2.9805,D(x): 0.83, D(G(z)): 0.10\n",
            "Epoch 8, batch 571, d_loss: 0.6345, g_loss: 2.9708,D(x): 0.77, D(G(z)): 0.14\n",
            "Epoch 9, batch 271, d_loss: 0.5179, g_loss: 2.4259,D(x): 0.81, D(G(z)): 0.20\n",
            "Epoch 9, batch 571, d_loss: 1.8039, g_loss: 0.9572,D(x): 0.49, D(G(z)): 0.48\n",
            "Epoch 10, batch 271, d_loss: 1.1355, g_loss: 1.3032,D(x): 0.69, D(G(z)): 0.42\n",
            "Epoch 10, batch 571, d_loss: 1.5439, g_loss: 1.1238,D(x): 0.50, D(G(z)): 0.39\n",
            "Epoch 11, batch 271, d_loss: 0.9519, g_loss: 1.6153,D(x): 0.59, D(G(z)): 0.23\n",
            "Epoch 11, batch 571, d_loss: 0.7319, g_loss: 2.0421,D(x): 0.74, D(G(z)): 0.25\n",
            "Epoch 12, batch 271, d_loss: 0.5792, g_loss: 2.6825,D(x): 0.80, D(G(z)): 0.20\n",
            "Epoch 12, batch 571, d_loss: 0.4427, g_loss: 3.0675,D(x): 0.83, D(G(z)): 0.12\n",
            "Epoch 13, batch 271, d_loss: 0.9382, g_loss: 1.8798,D(x): 0.76, D(G(z)): 0.37\n",
            "Epoch 13, batch 571, d_loss: 1.1683, g_loss: 2.8378,D(x): 0.69, D(G(z)): 0.29\n",
            "Epoch 14, batch 271, d_loss: 1.0951, g_loss: 3.7541,D(x): 0.72, D(G(z)): 0.27\n",
            "Epoch 14, batch 571, d_loss: 0.5196, g_loss: 2.5260,D(x): 0.80, D(G(z)): 0.17\n",
            "Epoch 15, batch 271, d_loss: 0.8871, g_loss: 2.0040,D(x): 0.72, D(G(z)): 0.24\n",
            "Epoch 15, batch 571, d_loss: 0.6753, g_loss: 2.2997,D(x): 0.83, D(G(z)): 0.27\n",
            "Epoch 16, batch 271, d_loss: 0.9704, g_loss: 1.9532,D(x): 0.77, D(G(z)): 0.38\n",
            "Epoch 16, batch 571, d_loss: 1.2842, g_loss: 1.2039,D(x): 0.72, D(G(z)): 0.49\n",
            "Epoch 17, batch 271, d_loss: 0.6501, g_loss: 2.5959,D(x): 0.81, D(G(z)): 0.27\n",
            "Epoch 17, batch 571, d_loss: 1.0893, g_loss: 1.9676,D(x): 0.60, D(G(z)): 0.26\n",
            "Epoch 18, batch 271, d_loss: 0.7514, g_loss: 2.8439,D(x): 0.69, D(G(z)): 0.14\n",
            "Epoch 18, batch 571, d_loss: 0.6410, g_loss: 2.4229,D(x): 0.82, D(G(z)): 0.27\n",
            "Epoch 19, batch 271, d_loss: 0.6993, g_loss: 1.9098,D(x): 0.73, D(G(z)): 0.19\n",
            "Epoch 19, batch 571, d_loss: 0.5059, g_loss: 2.2686,D(x): 0.81, D(G(z)): 0.16\n",
            "Epoch 20, batch 271, d_loss: 0.8638, g_loss: 1.5330,D(x): 0.76, D(G(z)): 0.32\n",
            "Epoch 20, batch 571, d_loss: 1.0939, g_loss: 2.2746,D(x): 0.69, D(G(z)): 0.30\n",
            "Epoch 21, batch 271, d_loss: 0.7068, g_loss: 2.2503,D(x): 0.75, D(G(z)): 0.20\n",
            "Epoch 21, batch 571, d_loss: 1.2973, g_loss: 1.3353,D(x): 0.67, D(G(z)): 0.34\n",
            "Epoch 22, batch 271, d_loss: 0.7980, g_loss: 1.9539,D(x): 0.76, D(G(z)): 0.27\n",
            "Epoch 22, batch 571, d_loss: 0.8227, g_loss: 1.9299,D(x): 0.75, D(G(z)): 0.31\n",
            "Epoch 23, batch 271, d_loss: 1.2091, g_loss: 1.3030,D(x): 0.68, D(G(z)): 0.41\n",
            "Epoch 23, batch 571, d_loss: 1.1624, g_loss: 1.9029,D(x): 0.55, D(G(z)): 0.22\n",
            "Epoch 24, batch 271, d_loss: 1.0398, g_loss: 1.5829,D(x): 0.70, D(G(z)): 0.32\n",
            "Epoch 24, batch 571, d_loss: 0.9108, g_loss: 2.4685,D(x): 0.71, D(G(z)): 0.31\n",
            "Epoch 25, batch 271, d_loss: 1.0850, g_loss: 1.6029,D(x): 0.72, D(G(z)): 0.38\n",
            "Epoch 25, batch 571, d_loss: 1.1346, g_loss: 2.8924,D(x): 0.63, D(G(z)): 0.28\n",
            "Epoch 26, batch 271, d_loss: 0.8672, g_loss: 1.7266,D(x): 0.71, D(G(z)): 0.26\n",
            "Epoch 26, batch 571, d_loss: 0.6825, g_loss: 2.1656,D(x): 0.69, D(G(z)): 0.16\n",
            "Epoch 27, batch 271, d_loss: 1.0253, g_loss: 2.0179,D(x): 0.67, D(G(z)): 0.27\n",
            "Epoch 27, batch 571, d_loss: 0.8403, g_loss: 1.8337,D(x): 0.68, D(G(z)): 0.21\n",
            "Epoch 28, batch 271, d_loss: 0.7910, g_loss: 1.9112,D(x): 0.76, D(G(z)): 0.30\n",
            "Epoch 28, batch 571, d_loss: 0.7271, g_loss: 1.9718,D(x): 0.74, D(G(z)): 0.23\n",
            "Epoch 29, batch 271, d_loss: 1.1986, g_loss: 1.9571,D(x): 0.68, D(G(z)): 0.35\n",
            "Epoch 29, batch 571, d_loss: 0.9454, g_loss: 1.4481,D(x): 0.73, D(G(z)): 0.34\n",
            "Epoch 30, batch 271, d_loss: 1.0069, g_loss: 2.3660,D(x): 0.76, D(G(z)): 0.37\n",
            "Epoch 30, batch 571, d_loss: 0.8821, g_loss: 1.4433,D(x): 0.72, D(G(z)): 0.29\n",
            "Epoch 31, batch 271, d_loss: 1.0918, g_loss: 1.5799,D(x): 0.71, D(G(z)): 0.37\n",
            "Epoch 31, batch 571, d_loss: 1.0812, g_loss: 1.8512,D(x): 0.73, D(G(z)): 0.37\n",
            "Epoch 32, batch 271, d_loss: 0.9495, g_loss: 2.2928,D(x): 0.67, D(G(z)): 0.27\n",
            "Epoch 32, batch 571, d_loss: 0.9324, g_loss: 1.4179,D(x): 0.70, D(G(z)): 0.33\n",
            "Epoch 33, batch 271, d_loss: 0.9331, g_loss: 1.9280,D(x): 0.62, D(G(z)): 0.18\n",
            "Epoch 33, batch 571, d_loss: 1.1630, g_loss: 1.7989,D(x): 0.56, D(G(z)): 0.20\n",
            "Epoch 34, batch 271, d_loss: 0.9154, g_loss: 1.6620,D(x): 0.68, D(G(z)): 0.27\n",
            "Epoch 34, batch 571, d_loss: 0.8672, g_loss: 1.8168,D(x): 0.61, D(G(z)): 0.17\n",
            "Epoch 35, batch 271, d_loss: 0.9181, g_loss: 1.8270,D(x): 0.64, D(G(z)): 0.23\n",
            "Epoch 35, batch 571, d_loss: 0.7888, g_loss: 2.2723,D(x): 0.77, D(G(z)): 0.30\n",
            "Epoch 36, batch 271, d_loss: 0.9651, g_loss: 1.5227,D(x): 0.68, D(G(z)): 0.30\n",
            "Epoch 36, batch 571, d_loss: 0.7804, g_loss: 1.8568,D(x): 0.72, D(G(z)): 0.24\n",
            "Epoch 37, batch 271, d_loss: 0.9306, g_loss: 1.7867,D(x): 0.65, D(G(z)): 0.25\n",
            "Epoch 37, batch 571, d_loss: 0.9849, g_loss: 1.2320,D(x): 0.71, D(G(z)): 0.34\n",
            "Epoch 38, batch 271, d_loss: 0.9879, g_loss: 1.3534,D(x): 0.74, D(G(z)): 0.40\n",
            "Epoch 38, batch 571, d_loss: 0.7633, g_loss: 1.7933,D(x): 0.73, D(G(z)): 0.27\n",
            "Epoch 39, batch 271, d_loss: 1.1125, g_loss: 1.2789,D(x): 0.66, D(G(z)): 0.35\n",
            "Epoch 39, batch 571, d_loss: 0.9657, g_loss: 1.9022,D(x): 0.64, D(G(z)): 0.26\n",
            "Epoch 40, batch 271, d_loss: 0.8598, g_loss: 1.4349,D(x): 0.70, D(G(z)): 0.31\n",
            "Epoch 40, batch 571, d_loss: 1.1071, g_loss: 1.3780,D(x): 0.64, D(G(z)): 0.34\n",
            "Epoch 41, batch 271, d_loss: 0.9997, g_loss: 1.7536,D(x): 0.61, D(G(z)): 0.27\n",
            "Epoch 41, batch 571, d_loss: 1.0220, g_loss: 1.3495,D(x): 0.59, D(G(z)): 0.23\n",
            "Epoch 42, batch 271, d_loss: 0.9048, g_loss: 1.6861,D(x): 0.64, D(G(z)): 0.25\n",
            "Epoch 42, batch 571, d_loss: 0.8203, g_loss: 1.4871,D(x): 0.76, D(G(z)): 0.30\n",
            "Epoch 43, batch 271, d_loss: 0.7942, g_loss: 1.8998,D(x): 0.73, D(G(z)): 0.26\n",
            "Epoch 43, batch 571, d_loss: 0.9486, g_loss: 2.4568,D(x): 0.63, D(G(z)): 0.15\n",
            "Epoch 44, batch 271, d_loss: 1.1801, g_loss: 1.9211,D(x): 0.66, D(G(z)): 0.34\n",
            "Epoch 44, batch 571, d_loss: 0.7371, g_loss: 1.7468,D(x): 0.71, D(G(z)): 0.22\n",
            "Epoch 45, batch 271, d_loss: 0.7585, g_loss: 1.4778,D(x): 0.75, D(G(z)): 0.29\n",
            "Epoch 45, batch 571, d_loss: 1.0040, g_loss: 1.3534,D(x): 0.70, D(G(z)): 0.37\n",
            "Epoch 46, batch 271, d_loss: 0.9684, g_loss: 1.6047,D(x): 0.64, D(G(z)): 0.25\n",
            "Epoch 46, batch 571, d_loss: 0.7985, g_loss: 2.1461,D(x): 0.70, D(G(z)): 0.22\n",
            "Epoch 47, batch 271, d_loss: 1.0117, g_loss: 1.6343,D(x): 0.65, D(G(z)): 0.32\n",
            "Epoch 47, batch 571, d_loss: 0.9550, g_loss: 1.3042,D(x): 0.80, D(G(z)): 0.44\n",
            "Epoch 48, batch 271, d_loss: 0.7455, g_loss: 1.6877,D(x): 0.72, D(G(z)): 0.25\n",
            "Epoch 48, batch 571, d_loss: 0.9994, g_loss: 1.1184,D(x): 0.69, D(G(z)): 0.33\n",
            "Epoch 49, batch 271, d_loss: 0.7507, g_loss: 1.8492,D(x): 0.76, D(G(z)): 0.27\n",
            "Epoch 49, batch 571, d_loss: 0.8398, g_loss: 1.4560,D(x): 0.78, D(G(z)): 0.33\n",
            "Epoch 50, batch 271, d_loss: 0.8023, g_loss: 1.7614,D(x): 0.77, D(G(z)): 0.32\n",
            "Epoch 50, batch 571, d_loss: 0.7942, g_loss: 1.7892,D(x): 0.72, D(G(z)): 0.26\n",
            "Epoch 51, batch 271, d_loss: 0.7734, g_loss: 1.8783,D(x): 0.76, D(G(z)): 0.31\n",
            "Epoch 51, batch 571, d_loss: 1.1689, g_loss: 1.6761,D(x): 0.57, D(G(z)): 0.26\n",
            "Epoch 52, batch 271, d_loss: 0.9354, g_loss: 1.9903,D(x): 0.78, D(G(z)): 0.37\n",
            "Epoch 52, batch 571, d_loss: 1.0360, g_loss: 1.4073,D(x): 0.67, D(G(z)): 0.32\n",
            "Epoch 53, batch 271, d_loss: 0.8753, g_loss: 1.3665,D(x): 0.77, D(G(z)): 0.35\n",
            "Epoch 53, batch 571, d_loss: 1.0663, g_loss: 1.5132,D(x): 0.70, D(G(z)): 0.35\n",
            "Epoch 54, batch 271, d_loss: 0.9463, g_loss: 1.3842,D(x): 0.76, D(G(z)): 0.36\n",
            "Epoch 54, batch 571, d_loss: 0.9820, g_loss: 1.5704,D(x): 0.69, D(G(z)): 0.32\n",
            "Epoch 55, batch 271, d_loss: 0.9108, g_loss: 1.8364,D(x): 0.63, D(G(z)): 0.21\n",
            "Epoch 55, batch 571, d_loss: 0.8603, g_loss: 1.7363,D(x): 0.72, D(G(z)): 0.30\n",
            "Epoch 56, batch 271, d_loss: 0.8733, g_loss: 1.9808,D(x): 0.73, D(G(z)): 0.29\n",
            "Epoch 56, batch 571, d_loss: 0.8583, g_loss: 1.5196,D(x): 0.76, D(G(z)): 0.34\n",
            "Epoch 57, batch 271, d_loss: 0.8002, g_loss: 1.8908,D(x): 0.68, D(G(z)): 0.21\n",
            "Epoch 57, batch 571, d_loss: 0.9548, g_loss: 1.5908,D(x): 0.73, D(G(z)): 0.34\n",
            "Epoch 58, batch 271, d_loss: 0.8497, g_loss: 1.5972,D(x): 0.74, D(G(z)): 0.30\n",
            "Epoch 58, batch 571, d_loss: 0.8440, g_loss: 1.9711,D(x): 0.73, D(G(z)): 0.27\n",
            "Epoch 59, batch 271, d_loss: 0.8133, g_loss: 1.5661,D(x): 0.70, D(G(z)): 0.26\n",
            "Epoch 59, batch 571, d_loss: 0.9685, g_loss: 1.3527,D(x): 0.71, D(G(z)): 0.35\n",
            "Epoch 60, batch 271, d_loss: 0.8584, g_loss: 1.7728,D(x): 0.69, D(G(z)): 0.26\n",
            "Epoch 60, batch 571, d_loss: 0.8809, g_loss: 2.0065,D(x): 0.67, D(G(z)): 0.21\n",
            "Epoch 61, batch 271, d_loss: 0.9806, g_loss: 1.4548,D(x): 0.68, D(G(z)): 0.30\n",
            "Epoch 61, batch 571, d_loss: 0.8142, g_loss: 1.7891,D(x): 0.78, D(G(z)): 0.32\n",
            "Epoch 62, batch 271, d_loss: 0.8002, g_loss: 1.5987,D(x): 0.77, D(G(z)): 0.32\n",
            "Epoch 62, batch 571, d_loss: 0.8715, g_loss: 1.5353,D(x): 0.67, D(G(z)): 0.24\n",
            "Epoch 63, batch 271, d_loss: 0.6574, g_loss: 2.1350,D(x): 0.79, D(G(z)): 0.26\n",
            "Epoch 63, batch 571, d_loss: 0.8185, g_loss: 2.1373,D(x): 0.70, D(G(z)): 0.25\n",
            "Epoch 64, batch 271, d_loss: 0.8270, g_loss: 1.7632,D(x): 0.69, D(G(z)): 0.24\n",
            "Epoch 64, batch 571, d_loss: 0.8443, g_loss: 1.3847,D(x): 0.75, D(G(z)): 0.33\n",
            "Epoch 65, batch 271, d_loss: 0.8245, g_loss: 1.7130,D(x): 0.73, D(G(z)): 0.27\n",
            "Epoch 65, batch 571, d_loss: 0.8081, g_loss: 1.6826,D(x): 0.72, D(G(z)): 0.25\n",
            "Epoch 66, batch 271, d_loss: 0.8249, g_loss: 1.6148,D(x): 0.72, D(G(z)): 0.25\n",
            "Epoch 66, batch 571, d_loss: 0.7808, g_loss: 1.7352,D(x): 0.71, D(G(z)): 0.27\n",
            "Epoch 67, batch 271, d_loss: 0.8493, g_loss: 1.9454,D(x): 0.68, D(G(z)): 0.24\n",
            "Epoch 67, batch 571, d_loss: 0.7435, g_loss: 1.6814,D(x): 0.79, D(G(z)): 0.31\n",
            "Epoch 68, batch 271, d_loss: 0.8386, g_loss: 2.3047,D(x): 0.66, D(G(z)): 0.20\n",
            "Epoch 68, batch 571, d_loss: 1.0122, g_loss: 1.9614,D(x): 0.59, D(G(z)): 0.22\n",
            "Epoch 69, batch 271, d_loss: 0.8065, g_loss: 1.3921,D(x): 0.69, D(G(z)): 0.23\n",
            "Epoch 69, batch 571, d_loss: 0.7902, g_loss: 1.9002,D(x): 0.73, D(G(z)): 0.27\n",
            "Epoch 70, batch 271, d_loss: 0.9106, g_loss: 2.0087,D(x): 0.65, D(G(z)): 0.24\n",
            "Epoch 70, batch 571, d_loss: 0.8604, g_loss: 1.4314,D(x): 0.72, D(G(z)): 0.28\n",
            "Epoch 71, batch 271, d_loss: 0.8027, g_loss: 1.1918,D(x): 0.73, D(G(z)): 0.27\n",
            "Epoch 71, batch 571, d_loss: 0.9127, g_loss: 1.4553,D(x): 0.79, D(G(z)): 0.38\n",
            "Epoch 72, batch 271, d_loss: 0.6454, g_loss: 1.8564,D(x): 0.74, D(G(z)): 0.21\n",
            "Epoch 72, batch 571, d_loss: 0.9063, g_loss: 1.6173,D(x): 0.71, D(G(z)): 0.30\n",
            "Epoch 73, batch 271, d_loss: 0.9719, g_loss: 1.5822,D(x): 0.69, D(G(z)): 0.31\n",
            "Epoch 73, batch 571, d_loss: 0.9462, g_loss: 1.7656,D(x): 0.63, D(G(z)): 0.26\n",
            "Epoch 74, batch 271, d_loss: 1.0076, g_loss: 1.6839,D(x): 0.72, D(G(z)): 0.35\n",
            "Epoch 74, batch 571, d_loss: 0.9543, g_loss: 1.4509,D(x): 0.71, D(G(z)): 0.34\n",
            "Epoch 75, batch 271, d_loss: 0.9608, g_loss: 1.4532,D(x): 0.78, D(G(z)): 0.37\n",
            "Epoch 75, batch 571, d_loss: 0.9197, g_loss: 1.6012,D(x): 0.64, D(G(z)): 0.23\n",
            "Epoch 76, batch 271, d_loss: 0.9024, g_loss: 1.5631,D(x): 0.75, D(G(z)): 0.36\n",
            "Epoch 76, batch 571, d_loss: 0.9360, g_loss: 2.5406,D(x): 0.68, D(G(z)): 0.28\n",
            "Epoch 77, batch 271, d_loss: 0.9208, g_loss: 1.7727,D(x): 0.64, D(G(z)): 0.23\n",
            "Epoch 77, batch 571, d_loss: 0.9881, g_loss: 1.7280,D(x): 0.73, D(G(z)): 0.36\n",
            "Epoch 78, batch 271, d_loss: 1.0356, g_loss: 1.5137,D(x): 0.78, D(G(z)): 0.42\n",
            "Epoch 78, batch 571, d_loss: 1.0301, g_loss: 1.9454,D(x): 0.76, D(G(z)): 0.40\n",
            "Epoch 79, batch 271, d_loss: 0.8536, g_loss: 1.7796,D(x): 0.76, D(G(z)): 0.32\n",
            "Epoch 79, batch 571, d_loss: 1.0469, g_loss: 1.5253,D(x): 0.73, D(G(z)): 0.39\n",
            "Epoch 80, batch 271, d_loss: 0.7607, g_loss: 1.8934,D(x): 0.71, D(G(z)): 0.22\n",
            "Epoch 80, batch 571, d_loss: 0.8590, g_loss: 1.4499,D(x): 0.73, D(G(z)): 0.30\n",
            "Epoch 81, batch 271, d_loss: 0.8456, g_loss: 1.5011,D(x): 0.71, D(G(z)): 0.29\n",
            "Epoch 81, batch 571, d_loss: 0.9774, g_loss: 1.7687,D(x): 0.70, D(G(z)): 0.32\n",
            "Epoch 82, batch 271, d_loss: 0.8722, g_loss: 1.7843,D(x): 0.67, D(G(z)): 0.24\n",
            "Epoch 82, batch 571, d_loss: 0.8152, g_loss: 1.7733,D(x): 0.69, D(G(z)): 0.25\n",
            "Epoch 83, batch 271, d_loss: 0.7903, g_loss: 1.6236,D(x): 0.77, D(G(z)): 0.31\n",
            "Epoch 83, batch 571, d_loss: 0.7869, g_loss: 1.9046,D(x): 0.68, D(G(z)): 0.22\n",
            "Epoch 84, batch 271, d_loss: 0.8565, g_loss: 1.5310,D(x): 0.72, D(G(z)): 0.30\n",
            "Epoch 84, batch 571, d_loss: 0.8290, g_loss: 1.4134,D(x): 0.73, D(G(z)): 0.29\n",
            "Epoch 85, batch 271, d_loss: 0.9604, g_loss: 1.8737,D(x): 0.68, D(G(z)): 0.29\n",
            "Epoch 85, batch 571, d_loss: 0.8315, g_loss: 1.8203,D(x): 0.77, D(G(z)): 0.32\n",
            "Epoch 86, batch 271, d_loss: 0.7736, g_loss: 1.7516,D(x): 0.81, D(G(z)): 0.34\n",
            "Epoch 86, batch 571, d_loss: 0.9482, g_loss: 1.4122,D(x): 0.74, D(G(z)): 0.33\n",
            "Epoch 87, batch 271, d_loss: 0.8374, g_loss: 1.8694,D(x): 0.69, D(G(z)): 0.25\n",
            "Epoch 87, batch 571, d_loss: 1.0124, g_loss: 1.5581,D(x): 0.72, D(G(z)): 0.33\n",
            "Epoch 88, batch 271, d_loss: 0.9375, g_loss: 2.0663,D(x): 0.68, D(G(z)): 0.29\n",
            "Epoch 88, batch 571, d_loss: 0.8891, g_loss: 1.4596,D(x): 0.69, D(G(z)): 0.28\n",
            "Epoch 89, batch 271, d_loss: 1.0206, g_loss: 1.7619,D(x): 0.60, D(G(z)): 0.22\n",
            "Epoch 89, batch 571, d_loss: 0.8567, g_loss: 1.5830,D(x): 0.71, D(G(z)): 0.29\n",
            "Epoch 90, batch 271, d_loss: 0.8778, g_loss: 1.4322,D(x): 0.73, D(G(z)): 0.32\n",
            "Epoch 90, batch 571, d_loss: 0.9549, g_loss: 1.9691,D(x): 0.70, D(G(z)): 0.30\n",
            "Epoch 91, batch 271, d_loss: 0.7644, g_loss: 2.0170,D(x): 0.76, D(G(z)): 0.27\n",
            "Epoch 91, batch 571, d_loss: 0.8083, g_loss: 1.3689,D(x): 0.72, D(G(z)): 0.25\n",
            "Epoch 92, batch 271, d_loss: 0.9217, g_loss: 1.8507,D(x): 0.66, D(G(z)): 0.27\n",
            "Epoch 92, batch 571, d_loss: 0.9729, g_loss: 1.2736,D(x): 0.73, D(G(z)): 0.36\n",
            "Epoch 93, batch 271, d_loss: 0.9252, g_loss: 1.4468,D(x): 0.73, D(G(z)): 0.35\n",
            "Epoch 93, batch 571, d_loss: 1.0073, g_loss: 1.1773,D(x): 0.77, D(G(z)): 0.41\n",
            "Epoch 94, batch 271, d_loss: 0.9330, g_loss: 1.4797,D(x): 0.72, D(G(z)): 0.33\n",
            "Epoch 94, batch 571, d_loss: 1.0062, g_loss: 1.6165,D(x): 0.76, D(G(z)): 0.40\n",
            "Epoch 95, batch 271, d_loss: 0.8347, g_loss: 1.5963,D(x): 0.64, D(G(z)): 0.21\n",
            "Epoch 95, batch 571, d_loss: 0.9316, g_loss: 1.8565,D(x): 0.68, D(G(z)): 0.26\n",
            "Epoch 96, batch 271, d_loss: 0.8948, g_loss: 1.6911,D(x): 0.64, D(G(z)): 0.25\n",
            "Epoch 96, batch 571, d_loss: 0.8801, g_loss: 1.4998,D(x): 0.72, D(G(z)): 0.34\n",
            "Epoch 97, batch 271, d_loss: 0.9951, g_loss: 1.5675,D(x): 0.64, D(G(z)): 0.28\n",
            "Epoch 97, batch 571, d_loss: 0.8626, g_loss: 1.9091,D(x): 0.70, D(G(z)): 0.26\n",
            "Epoch 98, batch 271, d_loss: 0.9316, g_loss: 1.6964,D(x): 0.70, D(G(z)): 0.30\n",
            "Epoch 98, batch 571, d_loss: 0.9870, g_loss: 1.5491,D(x): 0.72, D(G(z)): 0.33\n",
            "Epoch 99, batch 271, d_loss: 0.7211, g_loss: 1.7463,D(x): 0.71, D(G(z)): 0.21\n",
            "Epoch 99, batch 571, d_loss: 0.8797, g_loss: 1.5483,D(x): 0.76, D(G(z)): 0.33\n",
            "Epoch 100, batch 271, d_loss: 0.9971, g_loss: 1.5721,D(x): 0.63, D(G(z)): 0.26\n",
            "Epoch 100, batch 571, d_loss: 0.8324, g_loss: 1.7065,D(x): 0.68, D(G(z)): 0.22\n",
            "Epoch 101, batch 271, d_loss: 0.8646, g_loss: 1.7999,D(x): 0.78, D(G(z)): 0.34\n",
            "Epoch 101, batch 571, d_loss: 0.9843, g_loss: 1.4922,D(x): 0.68, D(G(z)): 0.32\n",
            "Epoch 102, batch 271, d_loss: 0.8253, g_loss: 1.7446,D(x): 0.78, D(G(z)): 0.36\n",
            "Epoch 102, batch 571, d_loss: 1.1032, g_loss: 1.8397,D(x): 0.68, D(G(z)): 0.34\n",
            "Epoch 103, batch 271, d_loss: 0.8971, g_loss: 1.6758,D(x): 0.74, D(G(z)): 0.34\n",
            "Epoch 103, batch 571, d_loss: 0.8078, g_loss: 1.6231,D(x): 0.70, D(G(z)): 0.27\n",
            "Epoch 104, batch 271, d_loss: 0.9139, g_loss: 1.3603,D(x): 0.72, D(G(z)): 0.32\n",
            "Epoch 104, batch 571, d_loss: 0.9649, g_loss: 1.7387,D(x): 0.71, D(G(z)): 0.33\n",
            "Epoch 105, batch 271, d_loss: 0.8815, g_loss: 1.7000,D(x): 0.70, D(G(z)): 0.27\n",
            "Epoch 105, batch 571, d_loss: 0.8264, g_loss: 1.4514,D(x): 0.77, D(G(z)): 0.34\n",
            "Epoch 106, batch 271, d_loss: 0.9547, g_loss: 1.7669,D(x): 0.68, D(G(z)): 0.31\n",
            "Epoch 106, batch 571, d_loss: 0.7851, g_loss: 1.4675,D(x): 0.75, D(G(z)): 0.30\n",
            "Epoch 107, batch 271, d_loss: 0.7597, g_loss: 2.1584,D(x): 0.74, D(G(z)): 0.24\n",
            "Epoch 107, batch 571, d_loss: 0.9286, g_loss: 1.7773,D(x): 0.76, D(G(z)): 0.35\n",
            "Epoch 108, batch 271, d_loss: 0.7874, g_loss: 2.1527,D(x): 0.75, D(G(z)): 0.27\n",
            "Epoch 108, batch 571, d_loss: 0.9908, g_loss: 1.7213,D(x): 0.66, D(G(z)): 0.25\n",
            "Epoch 109, batch 271, d_loss: 0.8169, g_loss: 1.8372,D(x): 0.72, D(G(z)): 0.26\n",
            "Epoch 109, batch 571, d_loss: 0.7524, g_loss: 1.3465,D(x): 0.75, D(G(z)): 0.29\n",
            "Epoch 110, batch 271, d_loss: 0.9577, g_loss: 1.5674,D(x): 0.65, D(G(z)): 0.27\n",
            "Epoch 110, batch 571, d_loss: 0.8204, g_loss: 1.8075,D(x): 0.63, D(G(z)): 0.16\n",
            "Epoch 111, batch 271, d_loss: 0.7552, g_loss: 1.4893,D(x): 0.77, D(G(z)): 0.30\n",
            "Epoch 111, batch 571, d_loss: 0.8725, g_loss: 1.8405,D(x): 0.74, D(G(z)): 0.32\n",
            "Epoch 112, batch 271, d_loss: 0.8311, g_loss: 1.8402,D(x): 0.71, D(G(z)): 0.26\n",
            "Epoch 112, batch 571, d_loss: 0.8732, g_loss: 1.4588,D(x): 0.70, D(G(z)): 0.28\n",
            "Epoch 113, batch 271, d_loss: 0.9212, g_loss: 1.5891,D(x): 0.74, D(G(z)): 0.35\n",
            "Epoch 113, batch 571, d_loss: 0.9381, g_loss: 1.4151,D(x): 0.72, D(G(z)): 0.31\n",
            "Epoch 114, batch 271, d_loss: 0.8261, g_loss: 1.5385,D(x): 0.80, D(G(z)): 0.35\n",
            "Epoch 114, batch 571, d_loss: 0.8528, g_loss: 2.0501,D(x): 0.66, D(G(z)): 0.21\n",
            "Epoch 115, batch 271, d_loss: 0.9307, g_loss: 1.1683,D(x): 0.77, D(G(z)): 0.40\n",
            "Epoch 115, batch 571, d_loss: 0.7907, g_loss: 1.5452,D(x): 0.70, D(G(z)): 0.26\n",
            "Epoch 116, batch 271, d_loss: 0.9058, g_loss: 1.1476,D(x): 0.74, D(G(z)): 0.30\n",
            "Epoch 116, batch 571, d_loss: 0.8476, g_loss: 1.7914,D(x): 0.70, D(G(z)): 0.23\n",
            "Epoch 117, batch 271, d_loss: 0.8716, g_loss: 2.1146,D(x): 0.70, D(G(z)): 0.25\n",
            "Epoch 117, batch 571, d_loss: 0.8690, g_loss: 1.3979,D(x): 0.77, D(G(z)): 0.35\n",
            "Epoch 118, batch 271, d_loss: 0.9806, g_loss: 1.6300,D(x): 0.67, D(G(z)): 0.30\n",
            "Epoch 118, batch 571, d_loss: 0.8591, g_loss: 1.5719,D(x): 0.71, D(G(z)): 0.31\n",
            "Epoch 119, batch 271, d_loss: 0.8523, g_loss: 1.5645,D(x): 0.74, D(G(z)): 0.31\n",
            "Epoch 119, batch 571, d_loss: 0.9859, g_loss: 2.1838,D(x): 0.67, D(G(z)): 0.26\n",
            "Epoch 120, batch 271, d_loss: 0.9720, g_loss: 1.6514,D(x): 0.66, D(G(z)): 0.28\n",
            "Epoch 120, batch 571, d_loss: 0.9012, g_loss: 2.1174,D(x): 0.64, D(G(z)): 0.21\n",
            "Epoch 121, batch 271, d_loss: 0.6788, g_loss: 1.7176,D(x): 0.77, D(G(z)): 0.26\n",
            "Epoch 121, batch 571, d_loss: 0.9250, g_loss: 1.8754,D(x): 0.73, D(G(z)): 0.29\n",
            "Epoch 122, batch 271, d_loss: 0.9480, g_loss: 1.8530,D(x): 0.62, D(G(z)): 0.24\n",
            "Epoch 122, batch 571, d_loss: 0.9215, g_loss: 1.8620,D(x): 0.81, D(G(z)): 0.37\n",
            "Epoch 123, batch 271, d_loss: 0.9294, g_loss: 1.4495,D(x): 0.70, D(G(z)): 0.32\n",
            "Epoch 123, batch 571, d_loss: 0.8246, g_loss: 1.3283,D(x): 0.72, D(G(z)): 0.30\n",
            "Epoch 124, batch 271, d_loss: 0.7395, g_loss: 1.7912,D(x): 0.69, D(G(z)): 0.19\n",
            "Epoch 124, batch 571, d_loss: 0.8319, g_loss: 1.6287,D(x): 0.70, D(G(z)): 0.27\n",
            "Epoch 125, batch 271, d_loss: 0.9267, g_loss: 1.9047,D(x): 0.70, D(G(z)): 0.33\n",
            "Epoch 125, batch 571, d_loss: 0.7901, g_loss: 1.7439,D(x): 0.75, D(G(z)): 0.30\n",
            "Epoch 126, batch 271, d_loss: 0.8657, g_loss: 1.9826,D(x): 0.64, D(G(z)): 0.19\n",
            "Epoch 126, batch 571, d_loss: 0.8363, g_loss: 1.9110,D(x): 0.72, D(G(z)): 0.26\n",
            "Epoch 127, batch 271, d_loss: 0.6834, g_loss: 2.2821,D(x): 0.77, D(G(z)): 0.25\n",
            "Epoch 127, batch 571, d_loss: 0.7891, g_loss: 1.8491,D(x): 0.72, D(G(z)): 0.24\n",
            "Epoch 128, batch 271, d_loss: 0.7608, g_loss: 2.0742,D(x): 0.77, D(G(z)): 0.28\n",
            "Epoch 128, batch 571, d_loss: 0.7645, g_loss: 1.5775,D(x): 0.81, D(G(z)): 0.32\n",
            "Epoch 129, batch 271, d_loss: 0.8883, g_loss: 1.7969,D(x): 0.68, D(G(z)): 0.28\n",
            "Epoch 129, batch 571, d_loss: 1.0046, g_loss: 1.7023,D(x): 0.84, D(G(z)): 0.45\n",
            "Epoch 130, batch 271, d_loss: 0.9217, g_loss: 2.0418,D(x): 0.73, D(G(z)): 0.31\n",
            "Epoch 130, batch 571, d_loss: 0.7880, g_loss: 1.4264,D(x): 0.68, D(G(z)): 0.21\n",
            "Epoch 131, batch 271, d_loss: 0.8202, g_loss: 1.8251,D(x): 0.72, D(G(z)): 0.24\n",
            "Epoch 131, batch 571, d_loss: 0.7846, g_loss: 1.6619,D(x): 0.80, D(G(z)): 0.33\n",
            "Epoch 132, batch 271, d_loss: 0.7926, g_loss: 1.8801,D(x): 0.74, D(G(z)): 0.25\n",
            "Epoch 132, batch 571, d_loss: 1.0118, g_loss: 1.7443,D(x): 0.73, D(G(z)): 0.34\n",
            "Epoch 133, batch 271, d_loss: 0.7470, g_loss: 1.9414,D(x): 0.72, D(G(z)): 0.22\n",
            "Epoch 133, batch 571, d_loss: 0.8445, g_loss: 1.9689,D(x): 0.77, D(G(z)): 0.32\n",
            "Epoch 134, batch 271, d_loss: 1.1361, g_loss: 1.9916,D(x): 0.58, D(G(z)): 0.23\n",
            "Epoch 134, batch 571, d_loss: 0.9219, g_loss: 1.7546,D(x): 0.65, D(G(z)): 0.20\n",
            "Epoch 135, batch 271, d_loss: 0.8062, g_loss: 1.7073,D(x): 0.80, D(G(z)): 0.35\n",
            "Epoch 135, batch 571, d_loss: 0.8180, g_loss: 2.0985,D(x): 0.74, D(G(z)): 0.27\n",
            "Epoch 136, batch 271, d_loss: 0.9519, g_loss: 1.4825,D(x): 0.71, D(G(z)): 0.33\n",
            "Epoch 136, batch 571, d_loss: 0.8757, g_loss: 1.6498,D(x): 0.68, D(G(z)): 0.24\n",
            "Epoch 137, batch 271, d_loss: 0.7246, g_loss: 1.6736,D(x): 0.72, D(G(z)): 0.22\n",
            "Epoch 137, batch 571, d_loss: 0.8986, g_loss: 1.8170,D(x): 0.70, D(G(z)): 0.29\n",
            "Epoch 138, batch 271, d_loss: 0.9417, g_loss: 1.5083,D(x): 0.72, D(G(z)): 0.32\n",
            "Epoch 138, batch 571, d_loss: 0.9738, g_loss: 1.6223,D(x): 0.64, D(G(z)): 0.22\n",
            "Epoch 139, batch 271, d_loss: 0.9727, g_loss: 2.1800,D(x): 0.60, D(G(z)): 0.20\n",
            "Epoch 139, batch 571, d_loss: 0.8629, g_loss: 1.7570,D(x): 0.76, D(G(z)): 0.34\n",
            "Epoch 140, batch 271, d_loss: 0.8117, g_loss: 1.5354,D(x): 0.78, D(G(z)): 0.34\n",
            "Epoch 140, batch 571, d_loss: 0.9159, g_loss: 1.5917,D(x): 0.70, D(G(z)): 0.30\n",
            "Epoch 141, batch 271, d_loss: 1.1744, g_loss: 1.5882,D(x): 0.63, D(G(z)): 0.30\n",
            "Epoch 141, batch 571, d_loss: 1.0085, g_loss: 1.3491,D(x): 0.72, D(G(z)): 0.32\n",
            "Epoch 142, batch 271, d_loss: 0.8821, g_loss: 1.6520,D(x): 0.72, D(G(z)): 0.31\n",
            "Epoch 142, batch 571, d_loss: 0.8450, g_loss: 1.7447,D(x): 0.68, D(G(z)): 0.23\n",
            "Epoch 143, batch 271, d_loss: 0.8727, g_loss: 2.0384,D(x): 0.67, D(G(z)): 0.24\n",
            "Epoch 143, batch 571, d_loss: 0.7966, g_loss: 1.4583,D(x): 0.80, D(G(z)): 0.34\n",
            "Epoch 144, batch 271, d_loss: 0.8334, g_loss: 1.2319,D(x): 0.76, D(G(z)): 0.33\n",
            "Epoch 144, batch 571, d_loss: 1.0106, g_loss: 1.8413,D(x): 0.71, D(G(z)): 0.35\n",
            "Epoch 145, batch 271, d_loss: 0.9114, g_loss: 1.4828,D(x): 0.73, D(G(z)): 0.31\n",
            "Epoch 145, batch 571, d_loss: 0.8241, g_loss: 1.6917,D(x): 0.76, D(G(z)): 0.32\n",
            "Epoch 146, batch 271, d_loss: 0.9954, g_loss: 1.6583,D(x): 0.62, D(G(z)): 0.24\n",
            "Epoch 146, batch 571, d_loss: 0.8969, g_loss: 1.6696,D(x): 0.79, D(G(z)): 0.35\n",
            "Epoch 147, batch 271, d_loss: 0.8808, g_loss: 1.9360,D(x): 0.73, D(G(z)): 0.31\n",
            "Epoch 147, batch 571, d_loss: 0.8139, g_loss: 1.9212,D(x): 0.77, D(G(z)): 0.31\n",
            "Epoch 148, batch 271, d_loss: 0.8198, g_loss: 1.4668,D(x): 0.69, D(G(z)): 0.23\n",
            "Epoch 148, batch 571, d_loss: 0.9090, g_loss: 1.6613,D(x): 0.81, D(G(z)): 0.39\n",
            "Epoch 149, batch 271, d_loss: 0.9265, g_loss: 1.8430,D(x): 0.72, D(G(z)): 0.32\n",
            "Epoch 149, batch 571, d_loss: 0.8523, g_loss: 1.8526,D(x): 0.68, D(G(z)): 0.24\n",
            "Epoch 150, batch 271, d_loss: 0.9436, g_loss: 1.6025,D(x): 0.72, D(G(z)): 0.31\n",
            "Epoch 150, batch 571, d_loss: 0.8707, g_loss: 1.5397,D(x): 0.80, D(G(z)): 0.36\n",
            "Epoch 151, batch 271, d_loss: 0.9213, g_loss: 2.0224,D(x): 0.70, D(G(z)): 0.29\n",
            "Epoch 151, batch 571, d_loss: 0.8805, g_loss: 1.8853,D(x): 0.73, D(G(z)): 0.31\n",
            "Epoch 152, batch 271, d_loss: 0.8003, g_loss: 1.6805,D(x): 0.79, D(G(z)): 0.32\n",
            "Epoch 152, batch 571, d_loss: 0.8648, g_loss: 1.7294,D(x): 0.67, D(G(z)): 0.23\n",
            "Epoch 153, batch 271, d_loss: 0.8315, g_loss: 1.6835,D(x): 0.74, D(G(z)): 0.31\n",
            "Epoch 153, batch 571, d_loss: 0.8908, g_loss: 1.8258,D(x): 0.75, D(G(z)): 0.32\n",
            "Epoch 154, batch 271, d_loss: 0.9102, g_loss: 2.0079,D(x): 0.66, D(G(z)): 0.24\n",
            "Epoch 154, batch 571, d_loss: 0.9241, g_loss: 1.4710,D(x): 0.75, D(G(z)): 0.33\n",
            "Epoch 155, batch 271, d_loss: 0.9402, g_loss: 1.6271,D(x): 0.72, D(G(z)): 0.34\n",
            "Epoch 155, batch 571, d_loss: 0.9391, g_loss: 1.9245,D(x): 0.75, D(G(z)): 0.33\n",
            "Epoch 156, batch 271, d_loss: 0.8834, g_loss: 1.8466,D(x): 0.71, D(G(z)): 0.28\n",
            "Epoch 156, batch 571, d_loss: 0.8891, g_loss: 1.8047,D(x): 0.71, D(G(z)): 0.26\n",
            "Epoch 157, batch 271, d_loss: 0.9256, g_loss: 1.9770,D(x): 0.76, D(G(z)): 0.34\n",
            "Epoch 157, batch 571, d_loss: 0.9866, g_loss: 1.6120,D(x): 0.72, D(G(z)): 0.35\n",
            "Epoch 158, batch 271, d_loss: 0.8133, g_loss: 2.0198,D(x): 0.72, D(G(z)): 0.24\n",
            "Epoch 158, batch 571, d_loss: 0.9079, g_loss: 1.3050,D(x): 0.76, D(G(z)): 0.33\n",
            "Epoch 159, batch 271, d_loss: 0.9257, g_loss: 1.7129,D(x): 0.81, D(G(z)): 0.38\n",
            "Epoch 159, batch 571, d_loss: 0.8406, g_loss: 1.5081,D(x): 0.76, D(G(z)): 0.32\n",
            "Epoch 160, batch 271, d_loss: 0.9260, g_loss: 1.7277,D(x): 0.62, D(G(z)): 0.17\n",
            "Epoch 160, batch 571, d_loss: 0.7749, g_loss: 1.4532,D(x): 0.73, D(G(z)): 0.27\n",
            "Epoch 161, batch 271, d_loss: 0.7703, g_loss: 1.6146,D(x): 0.78, D(G(z)): 0.27\n",
            "Epoch 161, batch 571, d_loss: 0.7690, g_loss: 1.9223,D(x): 0.73, D(G(z)): 0.27\n",
            "Epoch 162, batch 271, d_loss: 1.0827, g_loss: 1.4605,D(x): 0.80, D(G(z)): 0.43\n",
            "Epoch 162, batch 571, d_loss: 0.9172, g_loss: 2.3452,D(x): 0.62, D(G(z)): 0.21\n",
            "Epoch 163, batch 271, d_loss: 0.8044, g_loss: 1.7126,D(x): 0.72, D(G(z)): 0.28\n",
            "Epoch 163, batch 571, d_loss: 0.8325, g_loss: 1.6331,D(x): 0.75, D(G(z)): 0.31\n",
            "Epoch 164, batch 271, d_loss: 0.9841, g_loss: 1.5622,D(x): 0.68, D(G(z)): 0.27\n",
            "Epoch 164, batch 571, d_loss: 0.8640, g_loss: 1.4193,D(x): 0.75, D(G(z)): 0.32\n",
            "Epoch 165, batch 271, d_loss: 0.8494, g_loss: 2.1483,D(x): 0.76, D(G(z)): 0.30\n",
            "Epoch 165, batch 571, d_loss: 0.8759, g_loss: 2.1875,D(x): 0.68, D(G(z)): 0.23\n",
            "Epoch 166, batch 271, d_loss: 0.7333, g_loss: 2.1144,D(x): 0.72, D(G(z)): 0.22\n",
            "Epoch 166, batch 571, d_loss: 0.8592, g_loss: 1.7726,D(x): 0.73, D(G(z)): 0.30\n",
            "Epoch 167, batch 271, d_loss: 1.0369, g_loss: 1.7182,D(x): 0.70, D(G(z)): 0.34\n",
            "Epoch 167, batch 571, d_loss: 0.8450, g_loss: 1.8741,D(x): 0.74, D(G(z)): 0.30\n",
            "Epoch 168, batch 271, d_loss: 0.7023, g_loss: 1.6084,D(x): 0.82, D(G(z)): 0.30\n",
            "Epoch 168, batch 571, d_loss: 0.9239, g_loss: 2.1642,D(x): 0.72, D(G(z)): 0.29\n",
            "Epoch 169, batch 271, d_loss: 0.8708, g_loss: 1.8306,D(x): 0.69, D(G(z)): 0.25\n",
            "Epoch 169, batch 571, d_loss: 0.7549, g_loss: 1.6554,D(x): 0.76, D(G(z)): 0.26\n",
            "Epoch 170, batch 271, d_loss: 0.9142, g_loss: 1.6604,D(x): 0.72, D(G(z)): 0.28\n",
            "Epoch 170, batch 571, d_loss: 0.8665, g_loss: 1.8066,D(x): 0.72, D(G(z)): 0.29\n",
            "Epoch 171, batch 271, d_loss: 0.7652, g_loss: 1.9274,D(x): 0.70, D(G(z)): 0.22\n",
            "Epoch 171, batch 571, d_loss: 0.7275, g_loss: 2.0653,D(x): 0.70, D(G(z)): 0.18\n",
            "Epoch 172, batch 271, d_loss: 0.7513, g_loss: 1.7015,D(x): 0.74, D(G(z)): 0.25\n",
            "Epoch 172, batch 571, d_loss: 0.8868, g_loss: 2.0959,D(x): 0.70, D(G(z)): 0.27\n",
            "Epoch 173, batch 271, d_loss: 0.7775, g_loss: 2.0098,D(x): 0.73, D(G(z)): 0.24\n",
            "Epoch 173, batch 571, d_loss: 0.9255, g_loss: 1.8416,D(x): 0.69, D(G(z)): 0.29\n",
            "Epoch 174, batch 271, d_loss: 0.8728, g_loss: 1.6374,D(x): 0.67, D(G(z)): 0.25\n",
            "Epoch 174, batch 571, d_loss: 0.7648, g_loss: 1.9033,D(x): 0.76, D(G(z)): 0.26\n",
            "Epoch 175, batch 271, d_loss: 0.8172, g_loss: 2.0077,D(x): 0.75, D(G(z)): 0.29\n",
            "Epoch 175, batch 571, d_loss: 0.9783, g_loss: 1.9158,D(x): 0.67, D(G(z)): 0.28\n",
            "Epoch 176, batch 271, d_loss: 0.9341, g_loss: 1.9244,D(x): 0.72, D(G(z)): 0.31\n",
            "Epoch 176, batch 571, d_loss: 0.9157, g_loss: 1.8787,D(x): 0.68, D(G(z)): 0.25\n",
            "Epoch 177, batch 271, d_loss: 0.8757, g_loss: 1.6272,D(x): 0.67, D(G(z)): 0.23\n",
            "Epoch 177, batch 571, d_loss: 0.8514, g_loss: 1.8021,D(x): 0.75, D(G(z)): 0.29\n",
            "Epoch 178, batch 271, d_loss: 0.9023, g_loss: 1.8652,D(x): 0.67, D(G(z)): 0.28\n",
            "Epoch 178, batch 571, d_loss: 0.7827, g_loss: 1.9000,D(x): 0.75, D(G(z)): 0.22\n",
            "Epoch 179, batch 271, d_loss: 0.8833, g_loss: 1.8564,D(x): 0.64, D(G(z)): 0.22\n",
            "Epoch 179, batch 571, d_loss: 0.7513, g_loss: 1.8065,D(x): 0.79, D(G(z)): 0.29\n",
            "Epoch 180, batch 271, d_loss: 0.9217, g_loss: 1.9278,D(x): 0.61, D(G(z)): 0.18\n",
            "Epoch 180, batch 571, d_loss: 0.8139, g_loss: 1.9834,D(x): 0.74, D(G(z)): 0.28\n",
            "Epoch 181, batch 271, d_loss: 0.9550, g_loss: 1.6785,D(x): 0.76, D(G(z)): 0.32\n",
            "Epoch 181, batch 571, d_loss: 0.7762, g_loss: 1.9854,D(x): 0.71, D(G(z)): 0.24\n",
            "Epoch 182, batch 271, d_loss: 0.7226, g_loss: 1.6421,D(x): 0.77, D(G(z)): 0.29\n",
            "Epoch 182, batch 571, d_loss: 0.8921, g_loss: 1.5938,D(x): 0.74, D(G(z)): 0.28\n",
            "Epoch 183, batch 271, d_loss: 0.7328, g_loss: 2.0180,D(x): 0.72, D(G(z)): 0.22\n",
            "Epoch 183, batch 571, d_loss: 0.8347, g_loss: 1.5090,D(x): 0.76, D(G(z)): 0.30\n",
            "Epoch 184, batch 271, d_loss: 0.8330, g_loss: 1.6509,D(x): 0.70, D(G(z)): 0.25\n",
            "Epoch 184, batch 571, d_loss: 0.8408, g_loss: 1.6755,D(x): 0.72, D(G(z)): 0.28\n",
            "Epoch 185, batch 271, d_loss: 0.9123, g_loss: 1.5124,D(x): 0.80, D(G(z)): 0.39\n",
            "Epoch 185, batch 571, d_loss: 0.7961, g_loss: 1.7821,D(x): 0.74, D(G(z)): 0.29\n",
            "Epoch 186, batch 271, d_loss: 0.7697, g_loss: 2.0626,D(x): 0.76, D(G(z)): 0.30\n",
            "Epoch 186, batch 571, d_loss: 0.8446, g_loss: 1.5132,D(x): 0.71, D(G(z)): 0.29\n",
            "Epoch 187, batch 271, d_loss: 0.8126, g_loss: 1.9728,D(x): 0.72, D(G(z)): 0.25\n",
            "Epoch 187, batch 571, d_loss: 0.8717, g_loss: 1.6028,D(x): 0.80, D(G(z)): 0.35\n",
            "Epoch 188, batch 271, d_loss: 0.8889, g_loss: 1.4756,D(x): 0.77, D(G(z)): 0.34\n",
            "Epoch 188, batch 571, d_loss: 0.8120, g_loss: 1.5700,D(x): 0.73, D(G(z)): 0.28\n",
            "Epoch 189, batch 271, d_loss: 0.7726, g_loss: 1.9442,D(x): 0.68, D(G(z)): 0.19\n",
            "Epoch 189, batch 571, d_loss: 0.8385, g_loss: 1.7142,D(x): 0.76, D(G(z)): 0.30\n",
            "Epoch 190, batch 271, d_loss: 0.7982, g_loss: 1.9613,D(x): 0.73, D(G(z)): 0.25\n",
            "Epoch 190, batch 571, d_loss: 0.7711, g_loss: 1.7633,D(x): 0.72, D(G(z)): 0.24\n",
            "Epoch 191, batch 271, d_loss: 0.7691, g_loss: 1.5724,D(x): 0.74, D(G(z)): 0.27\n",
            "Epoch 191, batch 571, d_loss: 0.7475, g_loss: 1.7776,D(x): 0.79, D(G(z)): 0.31\n",
            "Epoch 192, batch 271, d_loss: 0.7500, g_loss: 1.8076,D(x): 0.76, D(G(z)): 0.28\n",
            "Epoch 192, batch 571, d_loss: 0.8638, g_loss: 1.4764,D(x): 0.74, D(G(z)): 0.31\n",
            "Epoch 193, batch 271, d_loss: 0.8282, g_loss: 1.8500,D(x): 0.69, D(G(z)): 0.22\n",
            "Epoch 193, batch 571, d_loss: 0.8722, g_loss: 2.1836,D(x): 0.72, D(G(z)): 0.30\n",
            "Epoch 194, batch 271, d_loss: 0.7362, g_loss: 2.0295,D(x): 0.73, D(G(z)): 0.24\n",
            "Epoch 194, batch 571, d_loss: 0.6852, g_loss: 2.0584,D(x): 0.70, D(G(z)): 0.16\n",
            "Epoch 195, batch 271, d_loss: 0.7900, g_loss: 1.4550,D(x): 0.81, D(G(z)): 0.31\n",
            "Epoch 195, batch 571, d_loss: 0.8840, g_loss: 1.7833,D(x): 0.72, D(G(z)): 0.28\n",
            "Epoch 196, batch 271, d_loss: 0.8329, g_loss: 1.7058,D(x): 0.79, D(G(z)): 0.34\n",
            "Epoch 196, batch 571, d_loss: 0.9312, g_loss: 2.0145,D(x): 0.74, D(G(z)): 0.29\n",
            "Epoch 197, batch 271, d_loss: 0.8517, g_loss: 1.8928,D(x): 0.70, D(G(z)): 0.27\n",
            "Epoch 197, batch 571, d_loss: 0.9174, g_loss: 1.6906,D(x): 0.77, D(G(z)): 0.35\n",
            "Epoch 198, batch 271, d_loss: 0.8375, g_loss: 1.6110,D(x): 0.71, D(G(z)): 0.27\n",
            "Epoch 198, batch 571, d_loss: 0.7186, g_loss: 1.6079,D(x): 0.75, D(G(z)): 0.24\n",
            "Epoch 199, batch 271, d_loss: 0.8058, g_loss: 1.8846,D(x): 0.79, D(G(z)): 0.32\n",
            "Epoch 199, batch 571, d_loss: 0.7318, g_loss: 1.6723,D(x): 0.81, D(G(z)): 0.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DisCriminatoR(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DisCriminatoR, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels=1,\n",
        "        out_channels=64,\n",
        "        kernel_size=5,\n",
        "        stride=2,\n",
        "        padding=2,\n",
        "        bias=True\n",
        "    )\n",
        "    self.leaky_relu = nn.LeakyReLU()\n",
        "    self.dropout_2d = nn.Dropout2d(0.3)\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        in_channels=64,\n",
        "        out_channels=128,\n",
        "        kernel_size=5,\n",
        "        stride=2,\n",
        "        padding=2,\n",
        "        bias=True\n",
        "    )\n",
        "    self.linear1 = nn.Linear(128*7*7, 1, bias=True)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.leaky_relu(out)\n",
        "    out = self.dropout_2d(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.leaky_relu(out)\n",
        "    out = self.dropout_2d(out)\n",
        "    out = out.view(-1, 128*7*7)\n",
        "    out = self.linear1(out)\n",
        "    out = self.sigmoid(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "_iOPwPXe_QJZ"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, latent_dim = 100, batchnorm=True):\n",
        "    super(Generator, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.batchnorm = batchnorm\n",
        "\n",
        "    self.linear1 = nn.Linear(latent_dim, 7*7*256, bias=False)\n",
        "    self.bn1d1 = nn.BatchNorm1d(7*7*256) if batchnorm else None\n",
        "    self.leaky_relu = nn.LeakyReLU()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels=256,\n",
        "        out_channels=128,\n",
        "        kernel_size=5,\n",
        "        stride=1,\n",
        "        padding=2,\n",
        "        bias=False\n",
        "    )\n",
        "\n",
        "    self.bn2d1 = nn.BatchNorm2d(128) if batchnorm else None\n",
        "    self.conv2 = nn.ConvTranspose2d(\n",
        "        in_channels=128,\n",
        "        out_channels=64,\n",
        "        kernel_size=4,\n",
        "        stride=2,\n",
        "        padding=1,\n",
        "        bias=False\n",
        "    )\n",
        "\n",
        "    self.bn2d2 = nn.BatchNorm2d(64) if batchnorm else None\n",
        "    self.conv3 = nn.ConvTranspose2d(\n",
        "        in_channels=64,\n",
        "        out_channels=1,\n",
        "        kernel_size=4,\n",
        "        stride=2,\n",
        "        padding=1,\n",
        "        bias=False\n",
        "    )\n",
        "\n",
        "    self.tanh = nn.Tanh()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.linear1(x)\n",
        "    if self.batchnorm:\n",
        "      out = self.bn1d1(out)\n",
        "    out = self.leaky_relu(out)\n",
        "    out = out.view(-1, 256, 7, 7)\n",
        "    out = self.conv1(out)\n",
        "\n",
        "    if self.batchnorm:\n",
        "      out = self.bn2d1(out)\n",
        "    out = self.leaky_relu(out)\n",
        "    out = self.conv2(out)\n",
        "\n",
        "    if self.batchnorm:\n",
        "      out = self.bn2d2(out)\n",
        "    out = self.leaky_relu(out)\n",
        "    out = self.conv3(out)\n",
        "\n",
        "    out = self.tanh(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "gPP0KeV6_QMG"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DCG = Generator()\n",
        "DCD = DisCriminatoR()"
      ],
      "metadata": {
        "id": "UD4NYaFnbelh"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available:\n",
        "  DCG.cuda()\n",
        "  DCD.cuda()"
      ],
      "metadata": {
        "id": "FGCSliiQjm0O"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dcd_opt = torch.optim.Adam(DCD.parameters(), lr=0.0002, betas=(0.5,0.999))\n",
        "dcg_opt = torch.optim.Adam(DCG.parameters(), lr=0.0002, betas=(0.5,0.999))"
      ],
      "metadata": {
        "id": "Ivlo6zD2iHa8"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for epoch in range(40):\n",
        "  for i, (images, _) in enumerate(data_loader):\n",
        "    batch_size = images.size(0)\n",
        "    images = to_var(images)\n",
        "\n",
        "    real_labels = to_var(torch.ones(batch_size, 1))\n",
        "    fake_labels = to_var(torch.zeros(batch_size, 1))\n",
        "\n",
        "    # ===================== Train DCD =====================\n",
        "    outputs = DCD(images)\n",
        "    d_loss_real = loss_fn(outputs, real_labels)\n",
        "    real_score = outputs\n",
        "\n",
        "    z = to_var(torch.randn(batch_size, 100))\n",
        "    fake_images = DCG(z) # Generate fake images\n",
        "    outputs = DCD(fake_images) # Discriminator's output on fake images\n",
        "    d_loss_fake = loss_fn(outputs, fake_labels)\n",
        "    fake_score = outputs\n",
        "\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "    DCD.zero_grad()\n",
        "    d_loss.backward()\n",
        "    dcd_opt.step()\n",
        "\n",
        "    # ===================== Train DCG =====================\n",
        "    z = to_var(torch.randn(batch_size, 100))\n",
        "    fake_images = DCG(z)\n",
        "    outputs = DCD(fake_images) # Discriminator's output on newly generated fake images\n",
        "\n",
        "    g_loss = loss_fn(outputs, real_labels)\n",
        "    DCD.zero_grad() # Clear gradients for DCD (even though we're training DCG, good practice)\n",
        "    DCG.zero_grad()\n",
        "    g_loss.backward()\n",
        "    dcg_opt.step()\n",
        "\n",
        "    if (i+30)%300 ==0 :\n",
        "      print(\"Epoch %d, batch %d, d_loss: %.4f, g_loss: %.4f,\"\n",
        "      \"D(x): %.2f, D(G(z)): %.2f\"\n",
        "      %(epoch, i+1, d_loss.data, g_loss.data,\n",
        "        real_score.data.mean(), fake_score.data.mean()))\n",
        "\n",
        "  # Create directory if not exists\n",
        "  if not os.path.exists('./data1'):\n",
        "    os.makedirs('./data1')\n",
        "\n",
        "  if (epoch ==0):\n",
        "    images = images.view(batch_size, 1, 28, 28)\n",
        "    save_image(denorm(images), \"./data1/real_images.png\")\n",
        "  fake_images = fake_images.view(fake_images.size(0), 1, 28, 28)\n",
        "  save_image(denorm(fake_images), \"./data1/fake_images-%d.png\"%(epoch+1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OMICyPtiHdB",
        "outputId": "694014bb-0612-4d6d-feba-eb92c51e5825"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, batch 271, d_loss: 1.2430, g_loss: 0.8829,D(x): 0.54, D(G(z)): 0.45\n",
            "Epoch 0, batch 571, d_loss: 1.3117, g_loss: 0.7868,D(x): 0.52, D(G(z)): 0.46\n",
            "Epoch 1, batch 271, d_loss: 1.3092, g_loss: 0.7959,D(x): 0.54, D(G(z)): 0.49\n",
            "Epoch 1, batch 571, d_loss: 1.3124, g_loss: 0.7757,D(x): 0.54, D(G(z)): 0.49\n",
            "Epoch 2, batch 271, d_loss: 1.3311, g_loss: 0.7434,D(x): 0.51, D(G(z)): 0.47\n",
            "Epoch 2, batch 571, d_loss: 1.2798, g_loss: 0.8129,D(x): 0.52, D(G(z)): 0.44\n",
            "Epoch 3, batch 271, d_loss: 1.2858, g_loss: 0.8180,D(x): 0.54, D(G(z)): 0.47\n",
            "Epoch 3, batch 571, d_loss: 1.3021, g_loss: 0.8229,D(x): 0.54, D(G(z)): 0.48\n",
            "Epoch 4, batch 271, d_loss: 1.2674, g_loss: 0.8622,D(x): 0.55, D(G(z)): 0.47\n",
            "Epoch 4, batch 571, d_loss: 1.2988, g_loss: 0.8576,D(x): 0.53, D(G(z)): 0.46\n",
            "Epoch 5, batch 271, d_loss: 1.3347, g_loss: 0.8604,D(x): 0.55, D(G(z)): 0.50\n",
            "Epoch 5, batch 571, d_loss: 1.2110, g_loss: 0.8440,D(x): 0.55, D(G(z)): 0.44\n",
            "Epoch 6, batch 271, d_loss: 1.2382, g_loss: 0.9251,D(x): 0.55, D(G(z)): 0.44\n",
            "Epoch 6, batch 571, d_loss: 1.2650, g_loss: 0.8365,D(x): 0.53, D(G(z)): 0.44\n",
            "Epoch 7, batch 271, d_loss: 1.2953, g_loss: 0.8007,D(x): 0.53, D(G(z)): 0.46\n",
            "Epoch 7, batch 571, d_loss: 1.2512, g_loss: 0.8426,D(x): 0.53, D(G(z)): 0.44\n",
            "Epoch 8, batch 271, d_loss: 1.2599, g_loss: 0.9082,D(x): 0.57, D(G(z)): 0.47\n",
            "Epoch 8, batch 571, d_loss: 1.3033, g_loss: 0.8448,D(x): 0.52, D(G(z)): 0.44\n",
            "Epoch 9, batch 271, d_loss: 1.3020, g_loss: 0.8000,D(x): 0.55, D(G(z)): 0.47\n",
            "Epoch 9, batch 571, d_loss: 1.2806, g_loss: 0.8737,D(x): 0.55, D(G(z)): 0.46\n",
            "Epoch 10, batch 271, d_loss: 1.3137, g_loss: 0.8175,D(x): 0.52, D(G(z)): 0.45\n",
            "Epoch 10, batch 571, d_loss: 1.2553, g_loss: 0.8075,D(x): 0.52, D(G(z)): 0.42\n",
            "Epoch 11, batch 271, d_loss: 1.3415, g_loss: 0.7723,D(x): 0.51, D(G(z)): 0.45\n",
            "Epoch 11, batch 571, d_loss: 1.2662, g_loss: 0.7505,D(x): 0.55, D(G(z)): 0.46\n",
            "Epoch 12, batch 271, d_loss: 1.3211, g_loss: 0.8511,D(x): 0.54, D(G(z)): 0.47\n",
            "Epoch 12, batch 571, d_loss: 1.3506, g_loss: 0.8030,D(x): 0.52, D(G(z)): 0.47\n",
            "Epoch 13, batch 271, d_loss: 1.2840, g_loss: 0.8985,D(x): 0.51, D(G(z)): 0.43\n",
            "Epoch 13, batch 571, d_loss: 1.2921, g_loss: 0.8399,D(x): 0.53, D(G(z)): 0.45\n",
            "Epoch 14, batch 271, d_loss: 1.2588, g_loss: 0.8200,D(x): 0.56, D(G(z)): 0.46\n",
            "Epoch 14, batch 571, d_loss: 1.3036, g_loss: 0.7679,D(x): 0.52, D(G(z)): 0.45\n",
            "Epoch 15, batch 271, d_loss: 1.2970, g_loss: 0.7640,D(x): 0.56, D(G(z)): 0.47\n",
            "Epoch 15, batch 571, d_loss: 1.3244, g_loss: 0.9247,D(x): 0.52, D(G(z)): 0.45\n",
            "Epoch 16, batch 271, d_loss: 1.2876, g_loss: 0.8568,D(x): 0.57, D(G(z)): 0.49\n",
            "Epoch 16, batch 571, d_loss: 1.3103, g_loss: 0.8563,D(x): 0.53, D(G(z)): 0.45\n",
            "Epoch 17, batch 271, d_loss: 1.2569, g_loss: 0.8261,D(x): 0.56, D(G(z)): 0.45\n",
            "Epoch 17, batch 571, d_loss: 1.2915, g_loss: 0.8355,D(x): 0.57, D(G(z)): 0.48\n",
            "Epoch 18, batch 271, d_loss: 1.2102, g_loss: 0.7120,D(x): 0.59, D(G(z)): 0.47\n",
            "Epoch 18, batch 571, d_loss: 1.2538, g_loss: 0.8372,D(x): 0.51, D(G(z)): 0.40\n",
            "Epoch 19, batch 271, d_loss: 1.3030, g_loss: 0.8042,D(x): 0.52, D(G(z)): 0.44\n",
            "Epoch 19, batch 571, d_loss: 1.2762, g_loss: 0.7845,D(x): 0.55, D(G(z)): 0.46\n",
            "Epoch 20, batch 271, d_loss: 1.2874, g_loss: 0.8959,D(x): 0.57, D(G(z)): 0.48\n",
            "Epoch 20, batch 571, d_loss: 1.2968, g_loss: 0.8341,D(x): 0.55, D(G(z)): 0.46\n",
            "Epoch 21, batch 271, d_loss: 1.3122, g_loss: 0.8519,D(x): 0.51, D(G(z)): 0.43\n",
            "Epoch 21, batch 571, d_loss: 1.2213, g_loss: 0.8606,D(x): 0.57, D(G(z)): 0.45\n",
            "Epoch 22, batch 271, d_loss: 1.3242, g_loss: 0.8255,D(x): 0.54, D(G(z)): 0.46\n",
            "Epoch 22, batch 571, d_loss: 1.2481, g_loss: 0.8485,D(x): 0.56, D(G(z)): 0.45\n",
            "Epoch 23, batch 271, d_loss: 1.3425, g_loss: 0.9268,D(x): 0.50, D(G(z)): 0.42\n",
            "Epoch 23, batch 571, d_loss: 1.2775, g_loss: 0.9115,D(x): 0.52, D(G(z)): 0.42\n",
            "Epoch 24, batch 271, d_loss: 1.3034, g_loss: 0.7982,D(x): 0.56, D(G(z)): 0.48\n",
            "Epoch 24, batch 571, d_loss: 1.1990, g_loss: 0.8452,D(x): 0.60, D(G(z)): 0.47\n",
            "Epoch 25, batch 271, d_loss: 1.2384, g_loss: 0.8246,D(x): 0.55, D(G(z)): 0.43\n",
            "Epoch 25, batch 571, d_loss: 1.3882, g_loss: 0.7731,D(x): 0.52, D(G(z)): 0.48\n",
            "Epoch 26, batch 271, d_loss: 1.3175, g_loss: 0.8554,D(x): 0.51, D(G(z)): 0.44\n",
            "Epoch 26, batch 571, d_loss: 1.2775, g_loss: 0.9308,D(x): 0.55, D(G(z)): 0.45\n",
            "Epoch 27, batch 271, d_loss: 1.2161, g_loss: 0.7722,D(x): 0.53, D(G(z)): 0.40\n",
            "Epoch 27, batch 571, d_loss: 1.2120, g_loss: 0.8665,D(x): 0.54, D(G(z)): 0.41\n",
            "Epoch 28, batch 271, d_loss: 1.2315, g_loss: 0.9637,D(x): 0.60, D(G(z)): 0.47\n",
            "Epoch 28, batch 571, d_loss: 1.2851, g_loss: 0.8325,D(x): 0.56, D(G(z)): 0.47\n",
            "Epoch 29, batch 271, d_loss: 1.2968, g_loss: 0.8400,D(x): 0.49, D(G(z)): 0.40\n",
            "Epoch 29, batch 571, d_loss: 1.2129, g_loss: 0.9983,D(x): 0.53, D(G(z)): 0.40\n",
            "Epoch 30, batch 271, d_loss: 1.2706, g_loss: 0.9364,D(x): 0.55, D(G(z)): 0.45\n",
            "Epoch 30, batch 571, d_loss: 1.2356, g_loss: 0.8056,D(x): 0.55, D(G(z)): 0.43\n",
            "Epoch 31, batch 271, d_loss: 1.2273, g_loss: 0.9338,D(x): 0.57, D(G(z)): 0.45\n",
            "Epoch 31, batch 571, d_loss: 1.2779, g_loss: 0.8548,D(x): 0.57, D(G(z)): 0.47\n",
            "Epoch 32, batch 271, d_loss: 1.2324, g_loss: 0.9112,D(x): 0.55, D(G(z)): 0.43\n",
            "Epoch 32, batch 571, d_loss: 1.2828, g_loss: 0.9975,D(x): 0.57, D(G(z)): 0.46\n",
            "Epoch 33, batch 271, d_loss: 1.1851, g_loss: 0.9063,D(x): 0.55, D(G(z)): 0.41\n",
            "Epoch 33, batch 571, d_loss: 1.3310, g_loss: 0.8844,D(x): 0.52, D(G(z)): 0.45\n",
            "Epoch 34, batch 271, d_loss: 1.2473, g_loss: 0.9450,D(x): 0.58, D(G(z)): 0.46\n",
            "Epoch 34, batch 571, d_loss: 1.3024, g_loss: 0.9400,D(x): 0.57, D(G(z)): 0.48\n",
            "Epoch 35, batch 271, d_loss: 1.2194, g_loss: 0.7940,D(x): 0.55, D(G(z)): 0.42\n",
            "Epoch 35, batch 571, d_loss: 1.2568, g_loss: 0.9326,D(x): 0.53, D(G(z)): 0.42\n",
            "Epoch 36, batch 271, d_loss: 1.2733, g_loss: 0.9183,D(x): 0.55, D(G(z)): 0.46\n",
            "Epoch 36, batch 571, d_loss: 1.2595, g_loss: 0.9248,D(x): 0.57, D(G(z)): 0.47\n",
            "Epoch 37, batch 271, d_loss: 1.1683, g_loss: 0.7442,D(x): 0.57, D(G(z)): 0.42\n",
            "Epoch 37, batch 571, d_loss: 1.2890, g_loss: 0.8156,D(x): 0.54, D(G(z)): 0.45\n",
            "Epoch 38, batch 271, d_loss: 1.1617, g_loss: 0.8970,D(x): 0.59, D(G(z)): 0.44\n",
            "Epoch 38, batch 571, d_loss: 1.2083, g_loss: 0.8556,D(x): 0.57, D(G(z)): 0.44\n",
            "Epoch 39, batch 271, d_loss: 1.2904, g_loss: 0.7818,D(x): 0.54, D(G(z)): 0.44\n",
            "Epoch 39, batch 571, d_loss: 1.3323, g_loss: 1.0085,D(x): 0.52, D(G(z)): 0.45\n"
          ]
        }
      ]
    }
  ]
}